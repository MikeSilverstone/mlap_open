%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Simple Sectioned Essay Template
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing essay content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article} % Default font size is 12pt, it can be changed here

\usepackage{geometry} % Required to change the page size to A4
\geometry{a4paper} % Set the page size to be A4 as opposed to the default US Letter

\usepackage{graphicx} % Required for including pictures

\usepackage{float} % Allows putting an [H] in \begin{figure} to specify the exact location of the figure
\usepackage{wrapfig} % Allows in-line images such as the example fish picture

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{float}
\restylefloat{table}

\linespread{1.2} % Line spacing

%\setlength\parindent{0pt} % Uncomment to remove all indentation from paragraphs

\graphicspath{{Pictures/}} % Specifies the directory where pictures are stored

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

\textsc{\LARGE University of York}\\[1.5cm] % Name of your university/college
\textsc{\Large MLAP}\\[0.5cm] % Major heading such as course name
\textsc{\large Machine Learning and Applications}\\[0.5cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Open Examination}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\center \emph{Examination number:}\\
\center Y3606797
\end{flushleft}
\end{minipage}

%\includegraphics{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\tableofcontents % Include a table of contents

\newpage % Begins the essay on a new page instead of on the same page as the table of contents 

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Task 1} % Major section

%------------------------------------------------

\subsection{What sort of model?} % Sub-section
The naive Bayes model makes the assumption that all features are conditionally independent given the class. The given Bayesian network explicitly models the conditional independence in it's structure.

\subsection{EM in general}
The EM algorithm is an iterative approach to maximising the likelihood when we have hidden variables. We have a model $p(\upsilon,h|\theta)$ and we wish to find $\theta$ by maximising the marginal likelihood $p(\upsilon|\theta)$. EM achieves this by replacing he marginal likelihood with a lower bound, the bound depends on $\theta$ and the set of variational distributions $\{q\}$, we optimise this bound. This is done by first fixing $\theta$ and then optimising \textit{w.r.t} $\{q\}$, then fixing $\{q\}$ and optimising \textit{w.r.t} $\theta$. These are known as the 'E' and 'M' steps and are repeated until convergence. The EM algorithm may not always return the correct MLEs because it can get stuck on local optima.

\subsection{EM for the current model}
Firstly we assume some initial parameters for the BN, so we have $\theta_{X1}^0$, $\theta_{X2}^0$, $\theta_{X3}^0$, $\theta_{H}^0$ and $\theta_{C}^0$. Then we compute the distribution for the hidden data H:
$$
q_{t=1}^{n=1}(H)=p(H|C=0,X1=0,X2=0,X3=1,\theta^0),
$$
$$
q_{t=1}^{n=2}(H)=p(H|C=1,X1=1,X2=0,X3=0,\theta^0)...
$$
and so on for each data point in the training data. This is first E-step in the algorithm.

The first M-step involves maximising the energy term by choosing new values for $\theta$. The energy term is given by the following equation:
\begin{multline}
E(\theta) = \displaystyle\sum_{n=1}^{N}
\{
\langle\log p(C^n|H^n) \rangle_{q_t^n(H)} +
\langle\log p(X1^n|H^n) \rangle_{q_t^n(H)} + 
\langle\log p(X2^n|H^n) \rangle_{q_t^n(H)} \\ +
\langle\log p(X3^n|H^n) \rangle_{q_t^n(H)} + 
\langle\log p(H^n) \rangle_{q_t^n(H)}
\}
\end{multline}

To calculate the M-step update for each table we have to maximise each term of the above equation individually. The contribution of the term $p(C=i|H=j)$ is given by:
$$
\displaystyle\sum_{n}\mathbb{I}[C^n=i]q^n(H=j) \log p(C=i|H=j))
$$
We normalise the table by adding a Lagrange term:
$$
\displaystyle\sum_{n}\mathbb{I}[C^n=i]q^n(H=j) \log p(C=i|H=j))
+ \lambda\{1-\sum_{k}p(C=k|H=j)\}
$$
We differentiate with respect to $p(C=i|H=j)$ and equate to zero to find the maximum, this gives us:
$$
\displaystyle\sum_{n}\mathbb{I}[C^n=i] \frac{q^n(H=j)}{p(C=i|H=j)} = \lambda
$$
Hence
$$
p(C=i|H=j)=
\frac{\sum_{n}\mathbb{I}[C^n=i]q^n(H=j)}{\sum_{n,k}\mathbb{I}[C^n=k]q^n(H=j)}
$$
This shows a relationship in the table updates, when we have missing data we replace the functions such as $\mathbb{I}[C^n=i]$ by the assumed distributions $q$.

We use this relationship to calculate the table updates for each other term in the energy equation.

We can then perform the E-step again using our updated parameters such that we have new assumed distributions for the hidden variable:
$$
q_{t}^{n=1}(H)=p(H|C=0,X1=0,X2=0,X3=1,\theta^{t-1}),
$$
$$
q_{t}^{n=2}(H)=p(H|C=1,X1=1,X2=0,X3=0,\theta^{t-1})...
$$
We perform the steps E and M iteratively until the algorithm converges to a local optima.
\end{document}